{"id": "Johnson2025-yr", "doi": "10.1371/journal.pcbi.1013758", "title": "Nucleotide context models outperform protein language models for predicting antibody affinity maturation", "authors": [{"first": "Mackenzie M", "last": "Johnson"}, {"first": "Kevin", "last": "Sung"}, {"first": "Hugh K", "last": "Haddox"}, {"first": "Ashni A", "last": "Vora"}, {"first": "Tatsuya", "last": "Araki"}, {"first": "Gabriel D", "last": "Victora"}, {"first": "Yun S", "last": "Song"}, {"first": "Julia", "last": "Fukuyama"}, {"first": "Frederick A", "last": "Matsen"}], "abstract": "Antibodies play a crucial role in adaptive immunity. They develop as B cell receptors (BCRs): membrane-bound forms of antibodies that are expressed on the surfaces of B cells. BCRs are refined through affinity maturation, a process of somatic hypermutation (SHM) and natural selection, to improve binding to an antigen. Computational models of affinity maturation have developed from two main perspectives: molecular evolution and language modeling. The molecular evolution perspective focuses on nucleotide sequence context to describe mutation and selection; the language modeling perspective involves learning patterns from large data sets of protein sequences. In this paper, we compared models from both perspectives on their ability to predict the course of antibody affinity maturation along phylogenetic trees of BCR sequences. This included models of SHM, models of SHM combined with an estimate of selection, and protein language models. We evaluated these models for large human BCR repertoire data sets, as well as an antigen-specific mouse experiment with a pre-rearranged cognate naive antibody. We demonstrated that precise modeling of SHM, which requires the nucleotide context, provides a substantial amount of predictive power for predicting the course of affinity maturation. Notably, a simple nucleotide-based convolutional neural network modeling SHM outperformed state-of-the-art protein language models, including one trained exclusively on antibody sequences. Furthermore, incorporating estimates of selection based on a custom deep mutational scanning experiment brought only modest improvement in predictive power. To support further research, we introduce EPAM (Evaluating Predictions of Affinity Maturation), a benchmarking framework to integrate evolutionary principles with advances in language modeling, offering a road map for understanding antibody evolution and improving predictive models.", "venue": "PLoS Comput. Biol.", "published": {"year": 2025, "month": 12, "day": 1}, "pdf_path": "All Papers/J/Johnson et al. 2025 - Nucleotide context models outperform protein language models for predicting antibody affinity maturation.pdf", "source": {"type": "paperpile", "id": "456fa37e-33a9-0115-af86-180ee2478252"}}
{"id": "Matsen2025-oj", "doi": "10.1101/2025.10.21.683652", "title": "Separating selection from mutation in antibody language models", "authors": [{"first": "Frederick A", "last": "Matsen"}, {"first": "Will", "last": "Dumm"}, {"first": "Kevin", "last": "Sung"}, {"first": "Mackenzie M", "last": "Johnson"}, {"first": "David", "last": "Rich"}, {"first": "Tyler", "last": "Starr"}, {"first": "Yun S", "last": "Song"}, {"first": "Julia", "last": "Fukuyama"}, {"first": "Hugh K", "last": "Haddox"}], "abstract": "Antibodies are encoded by nucleotide sequences that are generated by V(D)J recombination and evolve according to mutation and selection processes. Existing antibody language models, however, focus exclusively on antibodies as strings of amino acids and are fitted using standard language modeling objectives such as masked or autoregressive prediction. In this paper, we first show that fitting models using this objective implicitly incorporates nucleotide-level mutation processes as part of the protein language model, which degrades performance when predicting effects of mutations on functional properties of antibodies. To address this limitation, we devise a new framework: a Deep Amino acid Selection Model (DASM) that learns the selection effects of amino-acid mutations while explicitly factoring out the nucleotide-level mutation process. By fitting selection as a separate term from the mutation process, the DASM exclusively quantifies functional effects: effects that change some aspect of the function of the antibody. This factorization leads to substantially improved performance on standard functional benchmarks. Moreover, our model is an order of magnitude smaller and multiple orders of magnitude faster to evaluate than existing approaches, as well as being readily interpretable. ### Competing Interest Statement The authors have declared no competing interest. National Institutes of Health, https://ror.org/01cwqze88, AI146028, HG013117, AI177890 Howard Hughes Medical Institute, https://ror.org/006w34k90", "venue": "bioRxiv", "published": {"year": 2025, "month": 10, "day": 22}, "pdf_path": "All Papers/M/Matsen et al. 2025 - Separating selection from mutation in antibody language models.pdf", "source": {"type": "paperpile", "id": "3006ff77-c646-0b61-9f88-b7c33c6bfa86"}}
{"id": "Glanzer2025-td", "doi": "", "title": "Revealing bias in antibody language models through systematic training data processing with OAS-explore", "authors": [{"first": "Wiona Sophie", "last": "Gl\u00e4nzer"}, {"first": "Sai T", "last": "Reddy"}, {"first": "Alexander", "last": "Yermanos"}], "abstract": "Antibody language models (LMs) trained on immune receptor sequences have been applied to diverse immunological tasks such as humanization and prediction of antigen specificity. While promising, these models are often trained on datasets with limited donor diversity, raising concerns that biases in the training data may hinder their generalizability. To quantify the impact of biased training data, we introduce an open-source processing pipeline for the 2.4 billion unpaired antibody sequences in the Observed Antibody Space (OAS) database, enabling customizable filtering and balanced sampling by donor, species, chain type and other metadata. Analysis of OAS revealed that 13 individuals contribute over 70% of human antibody sequences. Using our pipeline, we trained 17 RoBERTa antibody LMs on datasets of different compositions. Models failed to generalize across chain types and showed limited transfer between human and mouse repertoires. Both individual- and batch-specific effects influenced model performance, and expanding donor diversity did not improve generalization to unseen individuals from unseen publications.", "venue": "", "published": {"year": 2025, "month": 10, "day": 6}, "pdf_path": "All Papers/G/Gl\u00e4nzer et al. 2025 - Revealing bias in antibody language models through systematic training data processing with OAS-explore.pdf", "source": {"type": "paperpile", "id": "d8c76a22-c88f-0b72-b3d2-1565dc8b808f"}}
{"id": "Sung2025-hz", "doi": "10.7554/eLife.105471", "title": "Thrifty wide-context models of B cell receptor somatic hypermutation", "authors": [{"first": "Kevin", "last": "Sung", "orcid": "0000-0002-7289-845X"}, {"first": "Mackenzie M", "last": "Johnson", "orcid": "0000-0002-3915-2023"}, {"first": "Will", "last": "Dumm"}, {"first": "Noah", "last": "Simon"}, {"first": "Hugh", "last": "Haddox"}, {"first": "Julia", "last": "Fukuyama", "orcid": "0000-0002-7590-5563"}, {"first": "Frederick A", "last": "Matsen", "orcid": "0000-0003-0607-6025"}], "abstract": "Somatic hypermutation (SHM) is the diversity-generating process in antibody affinity maturation. Probabilistic models of SHM are needed for analyzing rare mutations, understanding the selective forces guiding affinity maturation, and understanding the underlying biochemical process. High-throughput data offers the potential to develop and fit models of SHM on relevant data sets. In this article, we model SHM using modern frameworks. We are motivated by recent work suggesting the importance of a wider context for SHM; however, assigning an independent rate to each k-mer leads to an exponential proliferation of parameters. Thus, using convolutions on 3-mer embeddings, we develop 'thrifty' models of SHM of various sizes; these can have fewer free parameters than a 5-mer model and yet have a significantly wider context. These offer a slight performance improvement over a 5-mer model, and other modern model elaborations worsen performance. We also find that a per-site effect is not necessary to explain SHM patterns given nucleotide context. Also, the two current methods for fitting an SHM model-on out-of-frame sequence data and on synonymous mutations-produce significantly different results, and augmenting out-of-frame data with synonymous mutations does not aid out-of-sample performance.", "venue": "Elife", "published": {"year": 2025, "month": 8, "day": 29}, "pdf_path": "All Papers/S/Sung et al. 2025 - Thrifty wide-context models of B cell receptor somatic hypermutation.pdf", "source": {"type": "paperpile", "id": "f9276dab-1ffd-01cc-a2f0-0a11268653a6"}}
{"id": "Devlin2018-bd", "doi": "", "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding", "authors": [{"first": "Jacob", "last": "Devlin"}, {"first": "Ming-Wei", "last": "Chang"}, {"first": "Kenton", "last": "Lee"}, {"first": "Kristina", "last": "Toutanova"}], "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "venue": "arXiv [cs.CL]", "published": {"year": 2018, "month": 10, "day": 10}, "pdf_path": "All Papers/D/Devlin et al. 2018 - BERT - Pre-training of deep bidirectional Transformers for language understanding.pdf", "source": {"type": "paperpile", "id": "ededff09-07e4-449d-953a-95676d7afe0d"}}
{"id": "Ng2025-nf", "doi": "10.1016/j.patter.2025.101239", "title": "Focused learning by antibody language models using preferential masking of non-templated regions", "authors": [{"first": "Karenna", "last": "Ng"}, {"first": "Bryan", "last": "Briney"}], "abstract": "Existing antibody language models (AbLMs) are pre-trained using a masked language modeling (MLM) objective with uniform masking probabilities. While these models excel at predicting germline residues, they often struggle with mutated and non-templated residues, which concentrate in the complementarity-determining regions (CDRs) and are crucial for antigen binding specificity. Here, we demonstrate that preferential masking of the primarily non-templated CDR3 is a compute-efficient strategy to enhance model performance. We pre-trained two AbLMs using either uniform or preferential masking and observed that the latter improves residue prediction accuracy in the highly variable CDR3. Preferential masking also improves antibody classification by native chain pairing and binding specificity, suggesting improved CDR3 understanding and indicating that non-random, learnable patterns help govern antibody chain pairing. We further show that specificity classification is largely informed by residues in the CDRs, demonstrating that AbLMs learn meaningful patterns that align with immunological understanding.", "venue": "Patterns (N. Y.)", "published": {"year": 2025, "month": 6, "day": 13}, "pdf_path": "All Papers/N/Ng and Briney 2025 - Focused learning by antibody language models using preferential masking of non-templated regions.pdf", "source": {"type": "paperpile", "id": "8d1e9047-454d-0560-a4ad-b0671e9fa119"}}
{"id": "Leem2022-pd", "doi": "10.1016/j.patter.2022.100513", "title": "Deciphering the language of antibodies using self-supervised learning", "authors": [{"first": "Jinwoo", "last": "Leem"}, {"first": "Laura S", "last": "Mitchell"}, {"first": "James H R", "last": "Farmery"}, {"first": "Justin", "last": "Barton"}, {"first": "Jacob D", "last": "Galson"}], "abstract": "An individual's B cell receptor (BCR) repertoire encodes information about past immune responses and potential for future disease protection. Deciphering the information stored in BCR sequence datasets will transform our understanding of disease and enable discovery of novel diagnostics and antibody therapeutics. A key challenge of BCR sequence analysis is the prediction of BCR properties from their amino acid sequence alone. Here, we present an antibody-specific language model, Antibody-specific Bidirectional Encoder Representation from Transformers (AntiBERTa), which provides a contextualized representation of BCR sequences. Following pre-training, we show that AntiBERTa embeddings capture biologically relevant information, generalizable to a range of applications. As a case study, we fine-tune AntiBERTa to predict paratope positions from an antibody sequence, outperforming public tools across multiple metrics. To our knowledge, AntiBERTa is the deepest protein-family-specific language model, providing a rich representation of BCRs. AntiBERTa embeddings are primed for multiple downstream tasks and can improve our understanding of the language of antibodies.", "venue": "Patterns", "published": {"year": 2022, "month": 7, "day": 8}, "pdf_path": "All Papers/L/Leem et al. 2022 - Deciphering the language of antibodies using self-supervised learning.pdf", "source": {"type": "paperpile", "id": "504e7db5-9346-038b-bd26-5f0dd5da9fd4"}}
{"id": "Im2025-hi", "doi": "", "title": "SHIVER: Somatic Hypermutation Informed Vocabulary Encoder Representations", "authors": [{"first": "Chiho", "last": "Im"}, {"first": "Artem", "last": "Mikelov"}, {"first": "Ryan", "last": "Zhao"}, {"first": "Anshul", "last": "Kundaje"}, {"first": "Scott D", "last": "Boyd"}], "abstract": "Somatic hypermutations (SHMs) acquired during affinity maturation of memory B cell receptors (mBCRs) carry important immunological signals, but remain challenging for protein language models (PLMs) to capture effectively. We introduce SHIVER, a mutation-aware antibody language model that treats each amino acid substitution as a distinct token, allowing the model to directly encode the context-dependent impact of SHMs. Trained on paired heavy and light chain sequences from human mBCR repertoires, SHIVER incorporates a tailored vocabulary, mutation subsampling strategy, and partial masking scheme to better model the dynamics of affinity maturation. We evaluate SHIVER on the task of predicting mBCR binding to influenza antigens and find that it outperforms both general and antibody-specific PLMs using a simple logistic head. Our results suggest that explicitly modeling SHMs improves biological relevance and generalization of learned representations.", "venue": "", "published": {"year": 2025}, "pdf_path": "All Papers/I/Im et al. 2025 - SHIVER - Somatic Hypermutation Informed Vocabulary Encoder Representations.pdf", "source": {"type": "paperpile", "id": "4a785cfa-c761-093a-893b-d5e4e288f397"}}
{"id": "Chungyoun2024-fc", "doi": "10.1101/2024.01.13.575504", "title": "FLAb: Benchmarking deep learning methods for antibody fitness prediction", "authors": [{"first": "Michael", "last": "Chungyoun", "orcid": "0000-0002-8416-5966"}, {"first": "Jeffrey", "last": "Ruffolo", "orcid": "0000-0002-3385-9191"}, {"first": "Jeffrey", "last": "Gray", "orcid": "0000-0001-6380-2324"}], "abstract": "AbstractThe successful application of machine learning in therapeutic antibody design relies heavily on the ability of models to accurately represent the sequence-structure-function landscape, also known as the fitness landscape. Previous protein bench-marks (including The Critical Assessment of Function Annotation [33], Tasks Assessing Protein Embeddings [23], and FLIP [6]) examine fitness and mutational landscapes across many protein families, but they either exclude antibody data or use very little of it. In light of this, we present the Fitness Landscape for Antibodies (FLAb), the largest therapeutic antibody design benchmark to date. FLAb currently encompasses six properties of therapeutic antibodies: (1) expression, (2) thermosta-bility, (3) immunogenicity, (4) aggregation, (5) polyreactivity, and (6) binding affinity. We use FLAb to assess the performance of various widely adopted, pretrained, deep learning models for proteins (IgLM [28], AntiBERTy [26], ProtGPT2 [11], ProGen2 [21], ProteinMPNN [7], and ESM-IF [13]); and compare them to physics-based Rosetta [1]. Overall, no models are able to correlate with all properties or across multiple datasets of similar properties, indicating that more work is needed in prediction of antibody fitness. Additionally, we elucidate how wild type origin, deep learning architecture, training data composition, parameter size, and evolutionary signal affect performance, and we identify which fitness landscapes are more readily captured by each protein model. To promote an expansion on therapeutic antibody design benchmarking, all FLAb data are freely accessible and open for additional contribution athttps://github.com/Graylab/FLAb.", "venue": "bioRxiv", "published": {"year": 2024, "month": 1, "day": 15}, "pdf_path": "All Papers/C/Chungyoun et al. 2024 - FLAb - Benchmarking deep learning methods for antibody fitness prediction.pdf", "supplement_paths": ["All Papers/C/Chungyoun et al. 2024 - Chungyoun et al. 2024.table.pdf"], "source": {"type": "paperpile", "id": "b9c1988a-2f4a-0714-a844-9df0612758ce"}}
{"id": "Olsen2024-pu", "doi": "10.1093/bioinformatics/btae618", "title": "Addressing the antibody germline bias and its effect on language models for improved antibody design", "authors": [{"first": "Tobias H", "last": "Olsen", "orcid": "0000-0002-6348-4650"}, {"first": "Iain H", "last": "Moal"}, {"first": "Charlotte M", "last": "Deane", "orcid": "0000-0003-1388-2252"}], "abstract": "MOTIVATION: The versatile binding properties of antibodies have made them an extremely important class of biotherapeutics. However, therapeutic antibody development is a complex, expensive, and time-consuming task, with the final antibody needing to not only have strong and specific binding but also be minimally impacted by developability issues. The success of transformer-based language models in protein sequence space and the availability of vast amounts of antibody sequences, has led to the development of many antibody-specific language models to help guide antibody design. Antibody diversity primarily arises from V(D)J recombination, mutations within the CDRs, and/or from a few nongermline mutations outside the CDRs. Consequently, a significant portion of the variable domain of all natural antibody sequences remains germline. This affects the pre-training of antibody-specific language models, where this facet of the sequence data introduces a prevailing bias toward germline residues. This poses a challenge, as mutations away from the germline are often vital for generating specific and potent binding to a target, meaning that language models need be able to suggest key mutations away from germline. RESULTS: In this study, we explore the implications of the germline bias, examining its impact on both general-protein and antibody-specific language models. We develop and train a series of new antibody-specific language models optimized for predicting nongermline residues. We then compare our final model, AbLang-2, with current models and show how it suggests a diverse set of valid mutations with high cumulative probability. AVAILABILITY AND IMPLEMENTATION: AbLang-2 is trained on both unpaired and paired data, and is freely available at https://github.com/oxpig/AbLang2.git.", "venue": "Bioinformatics", "published": {"year": 2024, "month": 11, "day": 1}, "pdf_path": "All Papers/O/Olsen et al. 2024 - Addressing the antibody germline bias and its effect on language models for improved antibody design.pdf", "supplement_paths": ["All Papers/O/Olsen et al. 2024 - btae618_supplementary_data.pdf"], "source": {"type": "paperpile", "id": "6b4e1035-8aa8-055d-9535-7d350ce9891f"}}
{"id": "Ruffolo2021-ne", "doi": "", "title": "Deciphering antibody affinity maturation with language models and weakly supervised learning", "authors": [{"first": "Jeffrey A", "last": "Ruffolo"}, {"first": "Jeffrey J", "last": "Gray"}, {"first": "Jeremias", "last": "Sulam"}], "abstract": "In response to pathogens, the adaptive immune system generates specific antibodies that bind and neutralize foreign antigens. Understanding the composition of an individual's immune repertoire can provide insights into this process and reveal potential therapeutic antibodies. In this work, we explore the application of antibody-specific language models to aid understanding of immune repertoires. We introduce AntiBERTy, a language model trained on 558M natural antibody sequences. We find that within repertoires, our model clusters antibodies into trajectories resembling affinity maturation. Importantly, we show that models trained to predict highly redundant sequences under a multiple instance learning framework identify key binding residues in the process. With further development, the methods presented here will provide new insights into antigen binding from repertoire sequences alone.", "venue": "arXiv [q-bio.BM]", "published": {"year": 2021, "month": 12, "day": 14}, "pdf_path": "All Papers/R/Ruffolo et al. 2021 - Deciphering antibody affinity maturation with language models and weakly supervised learning.pdf", "source": {"type": "paperpile", "id": "5f73a0c2-e242-0c45-b019-8cc1b2dbb9e0"}}
{"id": "Hie2024-rz", "doi": "10.1038/s41587-023-01763-2", "title": "Efficient evolution of human antibodies from general protein language models", "authors": [{"first": "Brian L", "last": "Hie", "orcid": "0000-0003-3224-8142"}, {"first": "Varun R", "last": "Shanker", "orcid": "0000-0003-4443-9229"}, {"first": "Duo", "last": "Xu", "orcid": "0000-0003-0483-8719"}, {"first": "Theodora U J", "last": "Bruun", "orcid": "0000-0002-7462-2537"}, {"first": "Payton A", "last": "Weidenbacher", "orcid": "0000-0002-7692-0458"}, {"first": "Shaogeng", "last": "Tang", "orcid": "0000-0002-3904-492X"}, {"first": "Wesley", "last": "Wu", "orcid": "0000-0003-4594-0699"}, {"first": "John E", "last": "Pak"}, {"first": "Peter S", "last": "Kim", "orcid": "0000-0001-6503-4541"}], "abstract": "Natural evolution must explore a vast landscape of possible sequences for desirable yet rare mutations, suggesting that learning from natural evolutionary strategies could guide artificial evolution. Here we report that general protein language models can efficiently evolve human antibodies by suggesting mutations that are evolutionarily plausible, despite providing the model with no information about the target antigen, binding specificity or protein structure. We performed language-model-guided affinity maturation of seven antibodies, screening 20 or fewer variants of each antibody across only two rounds of laboratory evolution, and improved the binding affinities of four clinically relevant, highly mature antibodies up to sevenfold and three unmatured antibodies up to 160-fold, with many designs also demonstrating favorable thermostability and viral neutralization activity against Ebola and severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pseudoviruses. The same models that improve antibody binding also guide efficient evolution across diverse protein families and selection pressures, including antibiotic resistance and enzyme activity, suggesting that these results generalize to many settings.", "venue": "Nat. Biotechnol.", "published": {"year": 2024, "month": 2}, "pdf_path": "All Papers/H/Hie et al. 2024 - Efficient evolution of human antibodies from general protein language models.pdf", "source": {"type": "paperpile", "id": "ca061289-e1e8-0f02-a630-7949f7ebee2b"}}
{"id": "Lucaci2021-vh", "doi": "10.1371/journal.pone.0248337", "title": "Extra base hits: Widespread empirical support for instantaneous multiple-nucleotide changes", "authors": [{"first": "Alexander G", "last": "Lucaci", "orcid": "0000-0002-4896-6088"}, {"first": "Sadie R", "last": "Wisotsky"}, {"first": "Stephen D", "last": "Shank"}, {"first": "Steven", "last": "Weaver", "orcid": "0000-0002-6931-7191"}, {"first": "Sergei L", "last": "Kosakovsky Pond"}], "abstract": "Despite many attempts to introduce evolutionary models that permit substitutions to instantly alter more than one nucleotide in a codon, the prevailing wisdom remains that such changes are rare and generally negligible or are reflective of non-biological artifacts, such as alignment errors. Codon models continue to posit that only single nucleotide change have non-zero rates. Here, we develop and test a simple hierarchy of codon-substitution models with non-zero evolutionary rates for only one-nucleotide (1H), one- and two-nucleotide (2H), or any (3H) codon substitutions. Using over 42, 000 empirical alignments, we find widespread statistical support for multiple hits: 61% of alignments prefer models with 2H allowed, and 23%-with 3H allowed. Analyses of simulated data suggest that these results are not likely to be due to simple artifacts such as model misspecification or alignment errors. Further modeling reveals that synonymous codon island jumping among codons encoding serine, especially along short branches, contributes significantly to this 3H signal. While serine codons were prominently involved in multiple-hit substitutions, there were other common exchanges contributing to better model fit. It appears that a small subset of sites in most alignments have unusual evolutionary dynamics not well explained by existing model formalisms, and that commonly estimated quantities, such as dN/dS ratios may be biased by model misspecification. Our findings highlight the need for continued evaluation of assumptions underlying workhorse evolutionary models and subsequent evolutionary inference techniques. We provide a software implementation for evolutionary biologists to assess the potential impact of extra base hits in their data in the HyPhy package and in the Datamonkey.org server.", "venue": "PLoS One", "published": {"year": 2021, "month": 3, "day": 12}, "pdf_path": "All Papers/L/Lucaci et al. 2021 - Extra base hits - Widespread empirical support for instantaneous multiple-nucleotide changes.pdf", "source": {"type": "paperpile", "id": "52e41036-c270-0e4b-8c60-37547e95dade"}}
{"id": "Tang2022-iq", "doi": "10.1016/j.isci.2021.103668", "title": "Deep learning model of somatic hypermutation reveals importance of sequence context beyond hotspot targeting", "authors": [{"first": "Catherine", "last": "Tang"}, {"first": "Artem", "last": "Krantsevich"}, {"first": "Thomas", "last": "MacCarthy"}], "abstract": "B cells undergo somatic hypermutation (SHM) of the Immunoglobulin (Ig) variable region to generate high-affinity antibodies. SHM relies on the activity of activation-induced deaminase (AID), which mutates C>U preferentially targeting WRC (W=A/T, R=A/G) hotspots. Downstream mutations at WA Polymerase \u03b7 hotspots contribute further mutations. Computational models of SHM can describe the probability of mutations essential for vaccine responses. Previous studies using short subsequences (k-mers) failed to explain divergent mutability for the same k-mer. We developed the DeepSHM (Deep learning on SHM) model using k-mers of size 5-21, improving accuracy over previous models. Interpretation of DeepSHM identified an extended WWRCT motif with particularly high mutability. Increased mutability was further associated with lower surrounding G content. Our model also discovered a conserved AGYCTGGGGG (Y=C/T) motif within FW1 of IGHV3 family genes with unusually high T>G substitution rates. Thus, a wider sequence context increases predictive power and identifies features that drive mutational targeting.", "venue": "iScience", "published": {"year": 2022, "month": 1, "day": 21}, "pdf_path": "All Papers/T/Tang et al. 2022 - Deep learning model of somatic hypermutation reveals importance of sequence context beyond hotspot targeting.pdf", "source": {"type": "paperpile", "id": "c377a503-b7f1-0648-a852-2d3d6f853451"}}
{"id": "Spisak2020-zy", "doi": "10.1093/nar/gkaa825", "title": "Learning the heterogeneous hypermutation landscape of immunoglobulins from high-throughput repertoire data", "authors": [{"first": "Natanael", "last": "Spisak"}, {"first": "Aleksandra M", "last": "Walczak"}, {"first": "Thierry", "last": "Mora"}], "abstract": "Somatic hypermutations of immunoglobulin (Ig) genes occurring during affinity maturation drive B-cell receptors' ability to evolve strong binding to their antigenic targets. The landscape of these mutations is highly heterogeneous, with certain regions of the Ig gene being preferentially targeted. However, a rigorous quantification of this bias has been difficult because of phylogenetic correlations between sequences and the interference of selective forces. Here, we present an approach that corrects for these issues, and use it to learn a model of hypermutation preferences from a recently published large IgH repertoire dataset. The obtained model predicts mutation profiles accurately and in a reproducible way, including in the previously uncharacterized Complementarity Determining Region 3, revealing that both the sequence context of the mutation and its absolute position along the gene are important. In addition, we show that hypermutations occurring concomittantly along B-cell lineages tend to co-localize, suggesting a possible mechanism for accelerating affinity maturation.", "venue": "Nucleic Acids Res.", "published": {"year": 2020, "month": 11, "day": 4}, "pdf_path": "All Papers/S/Spisak et al. 2020 - Learning the heterogeneous hypermutation landscape of immunoglobulins from high-throughput repertoire data.pdf", "supplement_paths": ["All Papers/S/Spisak et al. 2020 - si_nar.pdf"], "source": {"type": "paperpile", "id": "e7b25132-6990-0103-8a14-768f080ecfd9"}}
